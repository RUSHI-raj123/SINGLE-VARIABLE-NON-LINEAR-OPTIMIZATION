{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyqxo/CB3Sk+PUJZUlqPpc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RUSHI-raj123/SINGLE-VARIABLE-NON-LINEAR-OPTIMIZATION/blob/main/SINGLE_VARIABLE_NON_LINEAR_OPTIMIZATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWH-qObfmUf6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def f(x):\n",
        "    return x**2 + 5\n",
        "\n",
        "def gradient(x):\n",
        "    return 2 * x\n",
        "\n",
        "def gradient_descent(initial_x, learning_rate, max_iterations, tolerance):\n",
        "    x = initial_x\n",
        "    history = []  # To store x and f(x) at each iteration\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        grad = gradient(x)\n",
        "\n",
        "        # Update x\n",
        "        x_new = x - learning_rate * grad\n",
        "\n",
        "        # Store current x and f(x)\n",
        "        history.append((x, f(x)))\n",
        "\n",
        "        # Check for convergence (if gradient is close to zero)\n",
        "        if abs(grad) < tolerance:\n",
        "            break\n",
        "\n",
        "        x = x_new\n",
        "\n",
        "    return x, f(x), history\n",
        "\n",
        "# Parameters\n",
        "initial_x = 2.0      # Initial guess\n",
        "learning_rate = 0.1  # Step size (m)\n",
        "max_iterations = 100 # Maximum iterations\n",
        "tolerance = 1e-6     # Stopping criterion (gradient close to zero)\n",
        "\n",
        "# Run Gradient Descent\n",
        "optimal_x, min_value, history = gradient_descent(initial_x, learning_rate, max_iterations, tolerance)\n",
        "\n",
        "# Results\n",
        "print(f\"Optimal x: {optimal_x}\")\n",
        "print(f\"Minimum value of f(x): {min_value}\")\n",
        "print(f\"Number of iterations: {len(history)}\")\n",
        "\n",
        "# Optional: Print history of x and f(x)\n",
        "print(\"\\nIteration History:\")\n",
        "for i, (x, fx) in enumerate(history):\n",
        "    print(f\"Iteration {i+1}: x = {x:.6f}, f(x) = {fx:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8fBKSi_JC_tn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}