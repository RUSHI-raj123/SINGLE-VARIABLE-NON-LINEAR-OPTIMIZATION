{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNz5GLLMzh7xBHtfrEVOaPz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RUSHI-raj123/SINGLE-VARIABLE-NON-LINEAR-OPTIMIZATION/blob/main/Lab_Exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChlJE3E1Urly"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Given dataset\n",
        "X = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.9, 0.8]\n",
        "Y = [3.3, 3.6, 3.9, 4.2, 4.5, 4.8, 5.1, 5.7, 5.4]\n",
        "\n",
        "# Initialize parameters\n",
        "m = random.random()  # Random initial slope\n",
        "c = random.random()  # Random initial intercept\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.1\n",
        "epochs = 100\n",
        "\n",
        "# Stochastic Gradient Descent\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle the data for each epoch (important for SGD)\n",
        "    combined = list(zip(X, Y))\n",
        "    random.shuffle(combined)\n",
        "    X_shuffled, Y_shuffled = zip(*combined)\n",
        "\n",
        "    for x, y_true in zip(X_shuffled, Y_shuffled):\n",
        "        # Prediction with current parameters\n",
        "        y_pred = m * x + c\n",
        "\n",
        "        # Compute gradients (partial derivatives)\n",
        "        dm = -2 * x * (y_true - y_pred)  # Derivative of MSE w.r.t. m\n",
        "        dc = -2 * (y_true - y_pred)      # Derivative of MSE w.r.t. c\n",
        "\n",
        "        # Update parameters\n",
        "        m = m - learning_rate * dm\n",
        "        c = c - learning_rate * dc\n",
        "\n",
        "    # Optional: Print progress (can be commented out)\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        total_error = sum((y - (m * x + c)) ** 2 for x, y in zip(X, Y)) / len(X)\n",
        "        print(f\"Epoch {epoch + 1}: m = {m:.4f}, c = {c:.4f}, MSE = {total_error:.4f}\")\n",
        "\n",
        "# Final results\n",
        "print(\"\\nFinal Parameters:\")\n",
        "print(f\"Slope (m): {m:.4f}\")\n",
        "print(f\"Intercept (c): {c:.4f}\")\n",
        "\n",
        "# Make predictions\n",
        "print(\"\\nPredictions:\")\n",
        "for x, y in zip(X, Y):\n",
        "    print(f\"X = {x:.1f}, True Y = {y:.1f}, Predicted Y = {(m * x + c):.1f}\")"
      ]
    }
  ]
}